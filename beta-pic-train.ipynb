{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\beta$ -Pic b planet\n",
    "\n",
    "I'll try to obtain the same posteriors as found in the paper of [Sun et al](https://arxiv.org/pdf/2201.08506.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prior import prior_distributions\n",
    "from simulator import OrbitCreator\n",
    "from training import train_sbi\n",
    "\n",
    "import torch\n",
    "\n",
    "import orbitize\n",
    "from orbitize import read_input\n",
    "\n",
    "from lampe.data import H5Dataset\n",
    "from lampe.inference import * # Import all methods and their losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the priors\n",
    "The priors are defined as such \n",
    "$$\n",
    "\\begin{align*}\n",
    "a &\\sim \\log \\mathcal{U}(10,10^{4})\\\\\n",
    "e &\\sim \\mathcal{U}(10^{8}, 0.99)\\\\\n",
    "i &\\sim \\mathcal{U}(0,180)\\\\\n",
    "\\omega &\\sim \\mathcal{U}(0,360)\\\\\n",
    "\\Omega &\\sim \\mathcal{U}(0,360)\\\\\n",
    "\\tau &\\sim \\mathcal{U}(0,1)\\\\\n",
    "\\pi &\\sim \\mathcal{N}(56.95, 0.26) \\\\\n",
    "M_T &\\sim \\mathcal{N}(1.22,0.08)\\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = prior_distributions(log_uniform_lower = torch.tensor(10.0), \n",
    "                            log_uniform_upper = torch.tensor(10**4),\n",
    "                            uniform_lower = torch.tensor([10e-8, 0.0, 0.0, 0.0, 0.0]), \n",
    "                            uniform_upper = torch.tensor([0.99, 180.0, 360.0, 360.0, 1.0]),\n",
    "                            gaussian_mean = torch.tensor([56.95, 1.22]), \n",
    "                            gaussian_std = torch.tensor([0.26, 0.08]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the simulator\n",
    "\n",
    "And test if it works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/matteo/Documents/SBI-astrometry/beta-pic-train.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matteo/Documents/SBI-astrometry/beta-pic-train.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m simulator \u001b[39m=\u001b[39m OrbitCreator(data_set)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matteo/Documents/SBI-astrometry/beta-pic-train.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m thetas \u001b[39m=\u001b[39m prior\u001b[39m.\u001b[39msample((\u001b[39m1\u001b[39m,))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matteo/Documents/SBI-astrometry/beta-pic-train.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m x \u001b[39m=\u001b[39m simulator(thetas)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_set' is not defined"
     ]
    }
   ],
   "source": [
    "simulator = OrbitCreator(data_set)\n",
    "\n",
    "thetas = prior.sample((1,))\n",
    "x = simulator(thetas)\n",
    "\n",
    "label_print = ['a', 'e', 'i', 'ω', 'Ω', 'τ', 'π', 'Mt']\n",
    "\n",
    "for label, theta_value in zip(label_print, thetas[0].tolist()):\n",
    "    print(f\"{label}: {theta_value:.3f}\")\n",
    "\n",
    "print(\"\\n x:\")\n",
    "for i in range(0, len(x[0]), 2):\n",
    "    x_coord = round(x[0][i].item(), 3)\n",
    "    y_coord = round(x[0][i+1].item(), 3)\n",
    "    print(f\"({x_coord}, {y_coord})\", end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = H5Dataset('Datasets/beta-pic-train.h5',\n",
    "                     batch_size=2048, shuffle=True)\n",
    "validset = H5Dataset('Datasets/beta-pic-val.h5',\n",
    "                     batch_size=2048)\n",
    "testset = H5Dataset('Datasets/beta-pic-test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the SBI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/128 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/128 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta shape: torch.Size([2048, 8])\n",
      "X shape: torch.Size([2048, 70])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4096x78 and 76x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/matteo/Documents/SBI-astrometry/beta-pic-train.ipynb Cell 14\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matteo/Documents/SBI-astrometry/beta-pic-train.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m len_obs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data_set) \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m# 2 coordinates per observation\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matteo/Documents/SBI-astrometry/beta-pic-train.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m estimator \u001b[39m=\u001b[39m NRE(\u001b[39m8\u001b[39m, len_obs, hidden_features\u001b[39m=\u001b[39m[\u001b[39m256\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m5\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matteo/Documents/SBI-astrometry/beta-pic-train.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_sbi(\u001b[39m'\u001b[39m\u001b[39mNRE\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matteo/Documents/SBI-astrometry/beta-pic-train.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m           estimator, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matteo/Documents/SBI-astrometry/beta-pic-train.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m           NRELoss, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matteo/Documents/SBI-astrometry/beta-pic-train.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m           trainset, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matteo/Documents/SBI-astrometry/beta-pic-train.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m           validset, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matteo/Documents/SBI-astrometry/beta-pic-train.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m           epochs \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/SBI-astrometry/training.py:58\u001b[0m, in \u001b[0;36mtrain_sbi\u001b[0;34m(name, estimator, estimator_loss, trainset, validset, epochs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTheta shape:\u001b[39m\u001b[39m\"\u001b[39m, theta\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     56\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mX shape:\u001b[39m\u001b[39m\"\u001b[39m, x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     57\u001b[0m     train_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([\n\u001b[0;32m---> 58\u001b[0m         step(loss(theta\u001b[39m.\u001b[39mcuda(), x\u001b[39m.\u001b[39mcuda()))\n\u001b[1;32m     59\u001b[0m     ])\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     61\u001b[0m estimator\u001b[39m.\u001b[39meval()\n\u001b[1;32m     62\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/tfe/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tfe/lib/python3.11/site-packages/lampe/inference.py:137\u001b[0m, in \u001b[0;36mNRELoss.forward\u001b[0;34m(self, theta, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mArguments:\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    theta: The parameters :math:`\\theta`, with shape :math:`(N, D)`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m    The scalar loss :math:`l`.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    135\u001b[0m theta_prime \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mroll(theta, \u001b[39m1\u001b[39m, dims\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m log_r, log_r_prime \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator(\n\u001b[1;32m    138\u001b[0m     torch\u001b[39m.\u001b[39mstack((theta, theta_prime)),\n\u001b[1;32m    139\u001b[0m     x,\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    142\u001b[0m l1 \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mF\u001b[39m.\u001b[39mlogsigmoid(log_r)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m    143\u001b[0m l0 \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mF\u001b[39m.\u001b[39mlogsigmoid(\u001b[39m-\u001b[39mlog_r_prime)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/anaconda3/envs/tfe/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tfe/lib/python3.11/site-packages/lampe/inference.py:103\u001b[0m, in \u001b[0;36mNRE.forward\u001b[0;34m(self, theta, x)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39mArguments:\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m    theta: The parameters :math:`\\theta`, with shape :math:`(*, D)`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m    The log-ratio :math:`\\log r_\\phi(\\theta, x)`, with shape :math:`(*,)`.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m theta, x \u001b[39m=\u001b[39m broadcast(theta, x, ignore\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(torch\u001b[39m.\u001b[39mcat((theta, x), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfe/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tfe/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfe/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tfe/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4096x78 and 76x256)"
     ]
    }
   ],
   "source": [
    "len_obs = len(data_set) * 2 # 2 coordinates per observation\n",
    "\n",
    "estimator = NRE(8, len_obs, hidden_features=[256]*5).cuda()\n",
    "\n",
    "train_sbi('NRE',\n",
    "          estimator, \n",
    "          NRELoss, \n",
    "          trainset, \n",
    "          validset, \n",
    "          epochs = 128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
